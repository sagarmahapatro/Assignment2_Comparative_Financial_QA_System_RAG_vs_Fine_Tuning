{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "DiiROl5BxvCK",
        "nWg6gZyqxyjr"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### BPE"
      ],
      "metadata": {
        "id": "DiiROl5BxvCK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenizer (including GPT-4o): https://tiktokenizer.vercel.app/?model=gpt2"
      ],
      "metadata": {
        "id": "bl4ekuUox-c8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken -q"
      ],
      "metadata": {
        "id": "rGb4CbOuy1SU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42911009-132d-48e6-b909-aa6cbc61f9f2"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Is the distance between Bengaluru and Delhi more than 2000 kms?\""
      ],
      "metadata": {
        "id": "9iX_4zbh3GkA"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "token_ids = tokenizer.encode(sentence)\n",
        "print(\"Token IDs:\", token_ids)\n",
        "decoded_tokens = [tokenizer.decode([token_id]) for token_id in token_ids]\n",
        "print(\"Tokens:\", decoded_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJICKUHB3cx4",
        "outputId": "ed5e4127-b93b-4bd4-ceda-fc8496cce344"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token IDs: [3792, 262, 5253, 1022, 28630, 14717, 290, 12517, 517, 621, 4751, 479, 907, 30]\n",
            "Tokens: ['Is', ' the', ' distance', ' between', ' Bengal', 'uru', ' and', ' Delhi', ' more', ' than', ' 2000', ' k', 'ms', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMbsBH-3eUEH",
        "outputId": "c754ef42-33f4-40da-975d-57eb4cbc96c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token IDs: [40, 679, 316, 6410, 316, 30076, 13, 15031, 382, 130805, 13, 2763, 290, 9324, 2870, 174589, 326, 30076, 220, 17713, 15, 109434, 30]\n",
            "Tokens: ['I', ' have', ' to', ' travel', ' to', ' Delhi', '.', ' Food', ' is', ' Delicious', '.', ' Is', ' the', ' distance', ' between', ' Bengaluru', ' and', ' Delhi', ' ', '280', '0', ' kms', '?']\n"
          ]
        }
      ],
      "source": [
        "import tiktoken\n",
        "tokenizer = tiktoken.get_encoding(\"o200k_base\") # GPT-4o mini\n",
        "token_ids = tokenizer.encode(sentence)\n",
        "print(\"Token IDs:\", token_ids)\n",
        "decoded_tokens = [tokenizer.decode([token_id]) for token_id in token_ids]\n",
        "print(\"Tokens:\", decoded_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode([64])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "NNksQnkobRJV",
        "outputId": "17390aa7-57ee-4e7f-c5b3-5afbf09fb388"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'a'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import unicodedata\n",
        "\n",
        "def get_stats(ids, counts=None):\n",
        "    counts = {} if counts is None else counts\n",
        "    for pair in zip(ids, ids[1:]):\n",
        "        counts[pair] = counts.get(pair, 0) + 1\n",
        "    return counts\n",
        "\n",
        "def merge(ids, pair, idx):\n",
        "    newids = []\n",
        "    i = 0\n",
        "    while i < len(ids):\n",
        "        if ids[i] == pair[0] and i < len(ids) - 1 and ids[i + 1] == pair[1]:\n",
        "            newids.append(idx)\n",
        "            i += 2\n",
        "        else:\n",
        "            newids.append(ids[i])\n",
        "            i += 1\n",
        "    return newids\n",
        "\n",
        "def train(text, vocab_size, verbose=False):\n",
        "    assert vocab_size >= 256\n",
        "    num_merges = vocab_size - 256\n",
        "    if verbose:\n",
        "      print(f'Text: {list(text)}')\n",
        "    text_bytes = text.encode(\"utf-8\")\n",
        "    ids = list(text_bytes)\n",
        "    if verbose:\n",
        "      print(f'ids: {ids}')\n",
        "    merges = {}\n",
        "    vocab = {idx: bytes([idx]) for idx in range(256)}\n",
        "    for i in range(num_merges):\n",
        "      if verbose:\n",
        "        print(f'\\nIteration {str(i+1)}:')\n",
        "      stats = get_stats(ids)\n",
        "      temp = [{(decode([int(k[0])],vocab),decode([int(k[1])],vocab)):v} for k,v in stats.items()]\n",
        "      if verbose:\n",
        "        print(f'Frequency of pairs: {temp}')\n",
        "      pair = max(stats, key=stats.get)\n",
        "      idx = 256 + i\n",
        "      if verbose:\n",
        "        print(f'Merging pairs: {(decode([int(pair[0])],vocab),decode([int(pair[1])],vocab))} ==> {idx}')\n",
        "      ids = merge(ids, pair, idx)\n",
        "      if verbose:\n",
        "        print(f'Merged ids: {ids}')\n",
        "      merges[pair] = idx\n",
        "      vocab[idx] = vocab[pair[0]] + vocab[pair[1]]\n",
        "      toks = [decode([int(id)],vocab) for id in ids]\n",
        "      if verbose:\n",
        "        print(f'Compressed Text: {toks}')\n",
        "    return merges, vocab\n",
        "\n",
        "def encode(text, merges):\n",
        "    text_bytes = text.encode(\"utf-8\")\n",
        "    ids = list(text_bytes)\n",
        "    while len(ids) >= 2:\n",
        "        stats = get_stats(ids)\n",
        "        pair = min(stats, key=lambda p: merges.get(p, float(\"inf\")))\n",
        "        if pair not in merges:\n",
        "            break\n",
        "        idx = merges[pair]\n",
        "        ids = merge(ids, pair, idx)\n",
        "    return ids\n",
        "\n",
        "def decode(ids, vocab):\n",
        "    text_bytes = b\"\".join(vocab[idx] for idx in ids)\n",
        "    text = text_bytes.decode(\"utf-8\", errors=\"replace\")\n",
        "    return text"
      ],
      "metadata": {
        "id": "bAk-DcxRA1Tu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mergesRequired=3\n",
        "merges, vocab = train('pay papaya', vocab_size = 256+mergesRequired, verbose=True)\n",
        "print(f'\\nMerged ids to new id: {list(merges.items())[-mergesRequired:]}')\n",
        "print(f'\\nNew vocabulary (id, byte):{list(vocab.items())[-mergesRequired:]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jU90dGcMnPVJ",
        "outputId": "7189bb12-fe98-4737-fe4b-858915f4ce18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: ['p', 'a', 'y', ' ', 'p', 'a', 'p', 'a', 'y', 'a']\n",
            "ids: [112, 97, 121, 32, 112, 97, 112, 97, 121, 97]\n",
            "\n",
            "Iteration 1:\n",
            "Frequency of pairs: [{('p', 'a'): 3}, {('a', 'y'): 2}, {('y', ' '): 1}, {(' ', 'p'): 1}, {('a', 'p'): 1}, {('y', 'a'): 1}]\n",
            "Merging pairs: ('p', 'a') ==> 256\n",
            "Merged ids: [256, 121, 32, 256, 256, 121, 97]\n",
            "Compressed Text: ['pa', 'y', ' ', 'pa', 'pa', 'y', 'a']\n",
            "\n",
            "Iteration 2:\n",
            "Frequency of pairs: [{('pa', 'y'): 2}, {('y', ' '): 1}, {(' ', 'pa'): 1}, {('pa', 'pa'): 1}, {('y', 'a'): 1}]\n",
            "Merging pairs: ('pa', 'y') ==> 257\n",
            "Merged ids: [257, 32, 256, 257, 97]\n",
            "Compressed Text: ['pay', ' ', 'pa', 'pay', 'a']\n",
            "\n",
            "Iteration 3:\n",
            "Frequency of pairs: [{('pay', ' '): 1}, {(' ', 'pa'): 1}, {('pa', 'pay'): 1}, {('pay', 'a'): 1}]\n",
            "Merging pairs: ('pay', ' ') ==> 258\n",
            "Merged ids: [258, 256, 257, 97]\n",
            "Compressed Text: ['pay ', 'pa', 'pay', 'a']\n",
            "\n",
            "Merged ids to new id: [((112, 97), 256), ((256, 121), 257), ((257, 32), 258)]\n",
            "\n",
            "New vocabulary (id, byte):[(256, b'pa'), (257, b'pay'), (258, b'pay ')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mergesRequired=3\n",
        "merges, vocab = train('25 pay 5 papaya', vocab_size = 256+mergesRequired, verbose=True)\n",
        "print(f'\\nMerged ids to new id: {list(merges.items())[-mergesRequired:]}')\n",
        "print(f'\\nNew vocabulary (id, byte):{list(vocab.items())[-mergesRequired:]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFu9ilK5rXKv",
        "outputId": "42f2f606-6cf4-4e07-cb6c-e45c1ee23b37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: ['2', '5', ' ', 'p', 'a', 'y', ' ', '5', ' ', 'p', 'a', 'p', 'a', 'y', 'a']\n",
            "ids: [50, 53, 32, 112, 97, 121, 32, 53, 32, 112, 97, 112, 97, 121, 97]\n",
            "\n",
            "Iteration 1:\n",
            "Frequency of pairs: [{('2', '5'): 1}, {('5', ' '): 2}, {(' ', 'p'): 2}, {('p', 'a'): 3}, {('a', 'y'): 2}, {('y', ' '): 1}, {(' ', '5'): 1}, {('a', 'p'): 1}, {('y', 'a'): 1}]\n",
            "Merging pairs: ('p', 'a') ==> 256\n",
            "Merged ids: [50, 53, 32, 256, 121, 32, 53, 32, 256, 256, 121, 97]\n",
            "Compressed Text: ['2', '5', ' ', 'pa', 'y', ' ', '5', ' ', 'pa', 'pa', 'y', 'a']\n",
            "\n",
            "Iteration 2:\n",
            "Frequency of pairs: [{('2', '5'): 1}, {('5', ' '): 2}, {(' ', 'pa'): 2}, {('pa', 'y'): 2}, {('y', ' '): 1}, {(' ', '5'): 1}, {('pa', 'pa'): 1}, {('y', 'a'): 1}]\n",
            "Merging pairs: ('5', ' ') ==> 257\n",
            "Merged ids: [50, 257, 256, 121, 32, 257, 256, 256, 121, 97]\n",
            "Compressed Text: ['2', '5 ', 'pa', 'y', ' ', '5 ', 'pa', 'pa', 'y', 'a']\n",
            "\n",
            "Iteration 3:\n",
            "Frequency of pairs: [{('2', '5 '): 1}, {('5 ', 'pa'): 2}, {('pa', 'y'): 2}, {('y', ' '): 1}, {(' ', '5 '): 1}, {('pa', 'pa'): 1}, {('y', 'a'): 1}]\n",
            "Merging pairs: ('5 ', 'pa') ==> 258\n",
            "Merged ids: [50, 258, 121, 32, 258, 256, 121, 97]\n",
            "Compressed Text: ['2', '5 pa', 'y', ' ', '5 pa', 'pa', 'y', 'a']\n",
            "\n",
            "Merged ids to new id: [((112, 97), 256), ((53, 32), 257), ((257, 256), 258)]\n",
            "\n",
            "New vocabulary (id, byte):[(256, b'pa'), (257, b'5 '), (258, b'5 pa')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Trained for Vocab of 512\n",
        "text = open(\"india.txt\", \"r\", encoding=\"utf-8\").read()\n",
        "merges, vocab = train(text, vocab_size=512, verbose=False)\n",
        "token_ids = encode(sentence, merges)\n",
        "tokens = [decode([tokenID], vocab) for tokenID in token_ids]\n",
        "print('Tokens generated:', tokens)\n",
        "print('Decoded sentence:', decode(token_ids, vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1L5k0jAL5Yt",
        "outputId": "539b42d3-d8c5-4c64-f493-f48003bda80c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens generated: ['I', 's ', 'the ', 'di', 'st', 'anc', 'e ', 'b', 'et', 'w', 'e', 'en ', 'B', 'en', 'g', 'al', 'ur', 'u', ' and ', 'D', 'el', 'hi', ' m', 'or', 'e ', 'th', 'an ', '20', '00 ', 'k', 'm', 's', '?']\n",
            "Decoded sentence: Is the distance between Bengaluru and Delhi more than 2000 kms?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Trained for Vocab of 1024\n",
        "text = open(\"india.txt\", \"r\", encoding=\"utf-8\").read()\n",
        "merges, vocab = train(text, vocab_size=1024, verbose=False)\n",
        "token_ids = encode(sentence, merges)\n",
        "tokens = [decode([tokenID], vocab) for tokenID in token_ids]\n",
        "print('Tokens generated:', tokens)\n",
        "print('Decoded sentence:', decode(token_ids, vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oikYNZZ-K1pL",
        "outputId": "73b552b7-2fec-4bbd-cb10-91fc39484645"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens generated: ['I', 's ', 'the ', 'di', 'st', 'ance ', 'between ', 'Beng', 'al', 'ur', 'u', ' and ', 'Delhi', ' m', 'or', 'e ', 'than ', '20', '00 ', 'k', 'm', 's', '?']\n",
            "Decoded sentence: Is the distance between Bengaluru and Delhi more than 2000 kms?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Difference between RegexTokeniser - GPT-3"
      ],
      "metadata": {
        "id": "sKLvtukw2O8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### WordPiece"
      ],
      "metadata": {
        "id": "nWg6gZyqxyjr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Likelihood score instead of frequency\n",
        "# standard special characters like !?@~\n",
        "# special tokens used for BERT, ex : [SEP], [CLS], [MASK], [UNK], [PAD], [EOS]"
      ],
      "metadata": {
        "id": "gJG5EGEI2FgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "# https://www.tensorflow.org/text/api_docs/python/text/BertTokenizer\n",
        "token_ids = tokenizer.encode(sentence)\n",
        "print(\"Token IDs:\", token_ids)\n",
        "decoded_tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
        "print(\"Tokens:\", decoded_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jE_lDdt56V-8",
        "outputId": "734fec5f-e496-420a-83ba-f030119ed0d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token IDs: [101, 2003, 1996, 3292, 2090, 8191, 14129, 1998, 6768, 13427, 2692, 2463, 2015, 1029, 102]\n",
            "Tokens: ['[CLS]', 'is', 'the', 'distance', 'between', 'bengal', '##uru', 'and', 'delhi', '280', '##0', 'km', '##s', '?', '[SEP]']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from collections import defaultdict\n",
        "\n",
        "def get_token_probabilities(vocab):\n",
        "    \"\"\"Calculates probabilities of tokens in the vocabulary.\n",
        "\n",
        "    Args:\n",
        "        vocab: A dictionary where keys are tokens (strings) and values are counts.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary where keys are tokens (strings) and values are their probabilities.\n",
        "    \"\"\"\n",
        "    total_count = sum(vocab.values())\n",
        "    return {token: count / total_count if total_count > 0 else 0 for token, count in vocab.items()} # Added a check to handle an empty vocabulary\n",
        "\n",
        "def get_stats_wordpiece(ids, vocab, vocab_id_reverse):\n",
        "    \"\"\"Calculates pair statistics for WordPiece, considering likelihood.\n",
        "\n",
        "    Args:\n",
        "        ids: A list of token IDs representing the text.\n",
        "        vocab: A dictionary of token counts {token_string: count}.\n",
        "        vocab_id_reverse: A dictionary for reverse lookup of id to token string {id: token_string}\n",
        "\n",
        "    Returns:\n",
        "        A dictionary where keys are pairs of token IDs and values are their likelihood scores.\n",
        "    \"\"\"\n",
        "    token_probabilities = get_token_probabilities(vocab)\n",
        "\n",
        "    pair_counts = defaultdict(int)\n",
        "    pair_scores = {}\n",
        "\n",
        "    for i in range(len(ids) - 1):\n",
        "        pair = (ids[i], ids[i + 1])\n",
        "        pair_counts[pair] += 1\n",
        "\n",
        "    for pair, count in pair_counts.items():\n",
        "        token1 = pair[0]\n",
        "        token2 = pair[1]\n",
        "        token1_str = vocab_id_reverse[token1] if isinstance(token1, int) else token1\n",
        "        token2_str = vocab_id_reverse[token2] if isinstance(token2, int) else token2\n",
        "\n",
        "        combined_token = token1_str + token2_str\n",
        "\n",
        "        prob_combined = vocab.get(combined_token, 0) / sum(vocab.values()) if vocab.get(combined_token, 0) != 0 else 1e-7\n",
        "        prob1 = token_probabilities.get(token1_str, 1e-7)\n",
        "        prob2 = token_probabilities.get(token2_str, 1e-7)\n",
        "\n",
        "        score = prob_combined / (prob1 * prob2) if prob1 != 0 and prob2 != 0 else 0\n",
        "\n",
        "        pair_scores[pair] = score\n",
        "\n",
        "    return pair_scores\n",
        "\n",
        "def merge_wordpiece(ids, pair, new_token, vocab, vocab_id_reverse):\n",
        "    \"\"\"Merges a pair of tokens in the list of IDs and Updates vocabulary.\n",
        "\n",
        "    Args:\n",
        "        ids: A list of token IDs representing the text.\n",
        "        pair: A tuple of two token IDs representing the pair to merge.\n",
        "        new_token: The new token string representing the merged pair.\n",
        "        vocab: The vocabulary dictionary {token_string: count}.\n",
        "        vocab_id_reverse: The vocabulary id reverse dictionary {id : token_string}.\n",
        "\n",
        "    Returns:\n",
        "        A new list of token IDs with the pair merged, and updated vocab and vocab_id_reverse.\n",
        "    \"\"\"\n",
        "    new_ids = []\n",
        "    i = 0\n",
        "    while i < len(ids):\n",
        "        if i < len(ids) - 1 and ids[i] == pair[0] and ids[i + 1] == pair[1]:\n",
        "            new_ids.append(new_token)  # Append new token string\n",
        "            i += 2\n",
        "        else:\n",
        "            new_ids.append(ids[i])\n",
        "            i += 1\n",
        "    # Add the merged tokens to vocab and vocab_id_reverse\n",
        "    if new_token not in vocab:\n",
        "        new_id = max(vocab_id_reverse.keys()) + 1 if vocab_id_reverse else 0\n",
        "        vocab[new_token] = 0 # Initialize count for new token\n",
        "    for i in range(len(new_ids)):\n",
        "        if(new_ids[i] == new_token):\n",
        "            vocab[new_token] +=1\n",
        "    if new_token not in vocab_id_reverse.values():\n",
        "      new_id = max(vocab_id_reverse.keys()) + 1 if vocab_id_reverse else 0\n",
        "      vocab_id_reverse[new_id] = new_token\n",
        "\n",
        "    new_ids_int = []\n",
        "    for i in range(len(new_ids)):\n",
        "        if isinstance(new_ids[i], int):\n",
        "            new_ids_int.append(new_ids[i])\n",
        "        else:\n",
        "            key = [key for key, value in vocab_id_reverse.items() if value == new_ids[i]]\n",
        "            if(key):\n",
        "                new_ids_int.append(key[0])\n",
        "            else:\n",
        "                print(\"Error\")\n",
        "\n",
        "    return new_ids_int, vocab, vocab_id_reverse\n",
        "\n",
        "def train_wordpiece(text, vocab_size, verbose=False):\n",
        "    \"\"\"Trains a WordPiece tokenizer.\n",
        "\n",
        "        Args:\n",
        "            text: The input text string.\n",
        "            vocab_size: The target vocabulary size.\n",
        "            verbose: Whether to print progress information.\n",
        "\n",
        "        Returns:\n",
        "            A tuple containing:\n",
        "              - merges: A dictionary where keys are pairs of token IDs and values\n",
        "              are the new token string they are merged into\n",
        "              - vocab: A dictionary of token counts {token_string: count}\n",
        "               - vocab_id_reverse : A dictionary of token id reverse map id: token\n",
        "        \"\"\"\n",
        "\n",
        "    # 1. Initial Vocabulary (with counts)\n",
        "    vocab = defaultdict(int)\n",
        "    for char in text:\n",
        "        vocab[char] += 1\n",
        "\n",
        "    # Initialize vocab_id_reverse here\n",
        "    vocab_id_reverse = {idx: char for idx, char in enumerate(vocab.keys())}\n",
        "\n",
        "    # 2. Initial IDs\n",
        "    ids = [ [key for key, value in vocab_id_reverse.items() if value == char ][0]for char in text]\n",
        "\n",
        "    merges = {}\n",
        "    num_merges = vocab_size - len(vocab)\n",
        "\n",
        "    for i in range(num_merges):\n",
        "        # 3. Get Pair Statistics (using likelihood)\n",
        "        pair_scores = get_stats_wordpiece(ids, vocab, vocab_id_reverse)\n",
        "        if not pair_scores:\n",
        "            break  # Stop if no more pairs can be merged\n",
        "\n",
        "        # 4. Find Best Pair\n",
        "        pair = max(pair_scores, key=pair_scores.get)\n",
        "\n",
        "        token1 = pair[0]\n",
        "        token2 = pair[1]\n",
        "        token1_str = vocab_id_reverse[token1] if isinstance(token1, int) else token1\n",
        "        token2_str = vocab_id_reverse[token2] if isinstance(token2, int) else token2\n",
        "\n",
        "        # 5. Create New Token\n",
        "        new_token = token1_str + token2_str\n",
        "\n",
        "        # 6. Merge IDs and Update Vocabulary\n",
        "        ids, vocab, vocab_id_reverse = merge_wordpiece(ids, pair, new_token, vocab, vocab_id_reverse)\n",
        "\n",
        "        # 7. Store Merge\n",
        "        merges[pair] = new_token\n",
        "\n",
        "        if verbose:\n",
        "            token1_for_print = vocab_id_reverse[pair[0]] if isinstance(pair[0],int) else pair[0]\n",
        "            token2_for_print = vocab_id_reverse[pair[1]] if isinstance(pair[1],int) else pair[1]\n",
        "            print(\n",
        "                f\"Merge {i + 1}/{num_merges}: ({token1_for_print}, {token2_for_print}) -> {new_token} (likelihood: {pair_scores[pair]:.4f})\"\n",
        "            )\n",
        "\n",
        "    return merges, vocab, vocab_id_reverse\n",
        "\n",
        "# Example Usage\n",
        "text = \"pay papaya pay papaya pay papaya pay papaya\"\n",
        "vocab_size = 5\n",
        "merges, vocab, vocab_id_reverse = train_wordpiece(text, vocab_size, verbose=True)\n",
        "print(\"Final Merges:\", merges)\n",
        "print(\"Final Vocab:\", vocab)\n",
        "print(\"Final Vocab reverse:\", vocab_id_reverse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "juk87ILdx0c0",
        "outputId": "1fa3ba0d-af91-4b2a-fcaa-3376a4aa88e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merge 1/26: (y,  ) -> y  (likelihood: 0.0000)\n",
            "Merge 2/26: (y , p) -> y p (likelihood: 0.0000)\n",
            "Merge 3/26: (a, y p) -> ay p (likelihood: 0.0000)\n",
            "Merge 4/26: (p, ay p) -> pay p (likelihood: 0.0000)\n",
            "Merge 5/26: ( , pay p) ->  pay p (likelihood: 0.0000)\n",
            "Merge 6/26: (a,  pay p) -> a pay p (likelihood: 0.0000)\n",
            "Merge 7/26: (y, a pay p) -> ya pay p (likelihood: 0.0000)\n",
            "Merge 8/26: (a, ya pay p) -> aya pay p (likelihood: 0.0000)\n",
            "Merge 9/26: (p, aya pay p) -> paya pay p (likelihood: 0.0000)\n",
            "Merge 10/26: (a, paya pay p) -> apaya pay p (likelihood: 0.0000)\n",
            "Merge 11/26: (apaya pay p, apaya pay p) -> apaya pay papaya pay p (likelihood: 0.0001)\n",
            "Merge 12/26: (apaya pay papaya pay p, apaya pay p) -> apaya pay papaya pay papaya pay p (likelihood: 0.0002)\n",
            "Merge 13/26: (pay p, apaya pay papaya pay papaya pay p) -> pay papaya pay papaya pay papaya pay p (likelihood: 0.0002)\n",
            "Merge 14/26: (pay papaya pay papaya pay papaya pay p, a) -> pay papaya pay papaya pay papaya pay pa (likelihood: 0.0000)\n",
            "Merge 15/26: (pay papaya pay papaya pay papaya pay pa, p) -> pay papaya pay papaya pay papaya pay pap (likelihood: 0.0001)\n",
            "Merge 16/26: (pay papaya pay papaya pay papaya pay pap, a) -> pay papaya pay papaya pay papaya pay papa (likelihood: 0.0000)\n",
            "Merge 17/26: (pay papaya pay papaya pay papaya pay papa, y) -> pay papaya pay papaya pay papaya pay papay (likelihood: 0.0001)\n",
            "Merge 18/26: (pay papaya pay papaya pay papaya pay papay, a) -> pay papaya pay papaya pay papaya pay papaya (likelihood: 0.0000)\n",
            "Final Merges: {(2, 3): 'y ', (4, 0): 'y p', (1, 5): 'ay p', (0, 6): 'pay p', (3, 7): ' pay p', (1, 8): 'a pay p', (2, 9): 'ya pay p', (1, 10): 'aya pay p', (0, 11): 'paya pay p', (1, 12): 'apaya pay p', (13, 13): 'apaya pay papaya pay p', (14, 13): 'apaya pay papaya pay papaya pay p', (7, 15): 'pay papaya pay papaya pay papaya pay p', (16, 1): 'pay papaya pay papaya pay papaya pay pa', (17, 0): 'pay papaya pay papaya pay papaya pay pap', (18, 1): 'pay papaya pay papaya pay papaya pay papa', (19, 2): 'pay papaya pay papaya pay papaya pay papay', (20, 1): 'pay papaya pay papaya pay papaya pay papaya'}\n",
            "Final Vocab: defaultdict(<class 'int'>, {'p': 12, 'a': 16, 'y': 8, ' ': 7, 'y ': 4, 'y p': 4, 'ay p': 4, 'pay p': 4, ' pay p': 3, 'a pay p': 3, 'ya pay p': 3, 'aya pay p': 3, 'paya pay p': 3, 'apaya pay p': 3, 'apaya pay papaya pay p': 1, 'apaya pay papaya pay papaya pay p': 1, 'pay papaya pay papaya pay papaya pay p': 1, 'pay papaya pay papaya pay papaya pay pa': 1, 'pay papaya pay papaya pay papaya pay pap': 1, 'pay papaya pay papaya pay papaya pay papa': 1, 'pay papaya pay papaya pay papaya pay papay': 1, 'pay papaya pay papaya pay papaya pay papaya': 1})\n",
            "Final Vocab reverse: {0: 'p', 1: 'a', 2: 'y', 3: ' ', 4: 'y ', 5: 'y p', 6: 'ay p', 7: 'pay p', 8: ' pay p', 9: 'a pay p', 10: 'ya pay p', 11: 'aya pay p', 12: 'paya pay p', 13: 'apaya pay p', 14: 'apaya pay papaya pay p', 15: 'apaya pay papaya pay papaya pay p', 16: 'pay papaya pay papaya pay papaya pay p', 17: 'pay papaya pay papaya pay papaya pay pa', 18: 'pay papaya pay papaya pay papaya pay pap', 19: 'pay papaya pay papaya pay papaya pay papa', 20: 'pay papaya pay papaya pay papaya pay papay', 21: 'pay papaya pay papaya pay papaya pay papaya'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformers"
      ],
      "metadata": {
        "id": "DajHVAVax1er"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "pggJu0xIx40w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12fcc325-5485-4c34-939b-a102f242e3e6"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2SdpaAttention(\n",
              "          (c_attn): Conv1D(nf=2304, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=768)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D(nf=3072, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=3072)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix = model.transformer.wte.weight\n",
        "embedding_matrix[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-iMaDyZaK5T",
        "outputId": "7398661a-6a2f-4a8d-f431-1be5d380da40"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-1.1010e-01, -3.9267e-02,  3.3108e-02,  1.3383e-01, -4.8476e-02,\n",
              "        -7.8918e-02, -2.3977e-01, -8.9474e-02,  2.5255e-02, -1.0740e-01,\n",
              "        -1.8115e-01, -6.7154e-02,  7.3914e-02, -1.6131e-02,  1.1662e-02,\n",
              "         1.2450e-01, -1.9630e-03, -8.1503e-02,  3.3778e-02,  2.3648e-01,\n",
              "        -7.7094e-02, -3.4552e-02, -2.3407e-02, -9.2770e-02,  1.3363e-01,\n",
              "        -6.6103e-02, -5.0176e-02,  4.6405e-02, -7.0965e-02, -9.0733e-02,\n",
              "         4.2812e-02, -8.8276e-02,  9.7773e-02, -1.3814e-01, -1.4034e-01,\n",
              "        -1.2564e-01, -3.1738e-01, -1.0916e-01, -7.0847e-02,  5.8407e-02,\n",
              "         7.8983e-03, -2.9746e-02,  1.4301e-01,  3.1137e-02, -3.1280e-02,\n",
              "         1.1017e-01,  5.4944e-02, -5.8925e-02,  3.5598e-02, -1.5325e-01,\n",
              "         1.5507e-02,  7.2158e-02,  2.7719e-02,  7.4660e-02,  1.3444e-01,\n",
              "        -1.2046e-01,  7.3460e-02,  1.3207e-01, -8.1152e-02,  9.0484e-02,\n",
              "        -1.0156e-01, -3.1801e-02,  7.5770e-02, -2.6173e-02, -1.7774e-01,\n",
              "         5.6041e-03, -2.7483e-02, -2.3783e-01,  1.7763e-01,  1.3464e-01,\n",
              "         2.5456e-02,  9.2345e-02,  1.1524e-02, -8.7020e-03, -1.7224e-01,\n",
              "        -2.1203e-02, -7.1322e-03,  1.8155e-01,  4.4082e-02, -1.3232e-01,\n",
              "        -5.1770e-02,  1.2584e-01,  3.1342e-03, -2.4966e-02,  7.5446e-02,\n",
              "         4.6427e-02, -2.0385e-01, -2.1180e-01,  1.4890e-01,  3.2461e-02,\n",
              "        -6.9528e-02,  1.8461e-02,  2.5883e-02,  5.1574e-03,  8.2476e-02,\n",
              "        -3.0313e-02, -5.0877e-02,  1.0797e-01, -4.1018e-03,  7.2734e-02,\n",
              "        -7.9695e-02, -7.1656e-02, -1.5103e-01, -4.9262e-02, -1.8729e-02,\n",
              "         1.1113e-01, -5.7040e-02, -1.5500e-01,  3.4637e-02, -7.8021e-03,\n",
              "        -2.8783e-02, -1.8501e-01, -4.4886e-02,  1.2235e-01,  4.5207e-03,\n",
              "         4.2003e-02,  2.5255e-01,  2.5061e-02, -1.0518e-01,  2.4897e-01,\n",
              "         4.4037e-03, -6.0401e-02,  5.4186e-02, -8.1150e-02,  6.5480e-02,\n",
              "         7.9328e-03, -5.8909e-02, -5.6703e-02,  3.4523e-03, -1.7040e-02,\n",
              "         1.4522e-01, -5.8785e-02,  2.3918e-02,  1.2605e-01, -1.0737e-02,\n",
              "        -1.4867e-01,  7.9653e-02,  6.5002e-02,  3.4652e-01, -4.1038e-02,\n",
              "        -1.7161e-01,  9.1625e-02,  1.1786e-01,  4.5274e-02, -7.1615e-02,\n",
              "        -1.3440e-01, -2.9960e-02,  1.9730e-01, -5.5200e-02,  8.5742e-02,\n",
              "        -7.3325e-03, -1.1093e-02, -6.3009e-02, -1.5695e-01,  4.3577e-05,\n",
              "        -1.6666e-02,  6.5562e-02, -1.0991e-01, -4.8620e-02, -1.0471e-01,\n",
              "        -8.6037e-02, -3.4725e-02,  5.5467e-02, -7.1422e-03, -3.3804e-02,\n",
              "        -6.3066e-02, -7.1277e-02,  1.6651e-01,  2.4361e-03, -3.8798e-02,\n",
              "         1.1853e-01, -6.0735e-02, -8.0287e-02,  2.0051e-02,  2.4760e-02,\n",
              "        -2.9060e-02,  4.0871e-01, -3.4364e-03,  1.5666e-02,  8.7188e-03,\n",
              "         3.6806e-02, -5.8686e-02,  5.0568e-02,  2.2303e-02,  2.6074e-02,\n",
              "         3.0044e-02,  1.0856e-01,  5.4271e-02, -1.1485e-02, -1.3249e-02,\n",
              "        -3.4294e-02,  1.4917e-01, -2.7654e-02, -3.3843e-02,  8.6916e-02,\n",
              "         9.9146e-02, -6.3963e-02, -4.2026e-02,  6.9346e-02,  5.0288e-02,\n",
              "         4.8356e-02,  5.9436e-02,  2.8662e-02,  7.2914e-02, -1.3495e-01,\n",
              "         6.0813e-03, -7.8744e-02,  2.2096e-01, -8.0342e-02, -4.5004e-02,\n",
              "         2.2359e-02,  9.3279e-02,  1.2621e-01, -1.8472e-01, -1.9790e-01,\n",
              "         5.5088e-03, -3.3463e-02, -1.5088e-02, -1.3279e-01,  3.8630e-02,\n",
              "         1.7160e-02,  8.4716e-02,  1.1958e-01,  1.3429e-03, -5.0427e-02,\n",
              "        -6.2631e-03,  6.5105e-02,  1.4172e-01, -4.8299e-02, -1.8803e-02,\n",
              "        -1.4914e-01,  2.2873e-02, -1.7010e-01, -1.4292e-01,  4.0731e-03,\n",
              "        -1.2844e-01, -5.3142e-02,  1.0153e-01, -1.2029e-01, -1.2745e-01,\n",
              "        -6.7966e-02,  1.4490e-02,  4.1670e-02,  1.5225e-01,  7.4801e-03,\n",
              "         8.9218e-02, -1.2545e-01,  1.5248e-02,  1.5385e-01,  5.5106e-02,\n",
              "         1.5300e-01, -5.8635e-02,  1.1060e-01,  6.5180e-02,  4.3578e-02,\n",
              "        -3.7659e-02,  1.9174e-02,  1.0162e-01,  7.3393e-02, -3.9101e-02,\n",
              "         6.5878e-02, -8.8858e-02,  5.4263e-02, -1.1379e-01, -3.4944e-03,\n",
              "        -3.9190e-02, -2.1554e-01, -1.8800e-02,  3.9101e-02,  6.2023e-02,\n",
              "         1.6452e-01, -1.2887e-01, -4.6402e-02,  1.1267e-01,  6.0487e-02,\n",
              "        -1.1963e-01,  5.9746e-02,  8.3203e-02, -2.5152e-03,  9.9375e-03,\n",
              "        -5.3293e-02,  6.5621e-03, -2.5560e-02, -3.4614e-02,  6.3118e-02,\n",
              "         2.4430e-02,  5.5635e-02, -2.0455e-02, -5.1772e-02,  8.6986e-02,\n",
              "        -8.3392e-02,  1.2621e-02, -9.3505e-02,  9.2645e-02,  1.0477e-01,\n",
              "        -1.2254e-01, -4.7425e-02, -6.8396e-02,  5.1940e-02, -4.7909e-02,\n",
              "         1.3788e-02, -4.0752e-02,  3.9935e-02,  1.0038e-01, -1.7514e-02,\n",
              "        -1.5064e-01,  6.3662e-02,  4.5459e-02,  1.4875e-01, -3.0464e-02,\n",
              "         1.7664e-01,  1.8426e-01,  3.2183e-02,  1.4997e-01,  1.7920e-01,\n",
              "         2.1068e-02, -2.3404e-01,  1.1247e-01,  7.8903e-02, -6.1902e-03,\n",
              "         1.8637e-01, -7.2527e-02,  4.7568e-03, -1.4132e-01,  3.1831e-02,\n",
              "        -8.9889e-02, -1.3114e-01,  8.4250e-02, -1.3648e-01,  3.1986e-02,\n",
              "        -8.8914e-02,  2.6627e-02,  1.3137e-01, -3.8133e-02,  1.2653e-01,\n",
              "        -1.7233e-02, -3.0606e-02,  2.2204e-02,  1.3117e-02,  1.2091e-01,\n",
              "         1.1133e-01,  9.6645e-02, -1.4791e-01, -5.0302e-02,  2.3090e-02,\n",
              "         1.0538e-02, -9.9736e-02,  1.3543e-01,  4.2334e-02,  2.6168e-02,\n",
              "        -4.3091e-02, -6.0693e-02,  6.0751e-02,  1.5326e-03,  4.3464e-02,\n",
              "        -6.6411e-02,  1.1155e-01, -1.4400e-01, -2.3725e-02,  7.6525e-02,\n",
              "         1.4678e-01, -2.1050e-02, -5.9657e-01, -9.7204e-02, -2.4771e-02,\n",
              "         1.3134e-01,  4.2053e-02,  7.6019e-02, -1.8450e-01, -1.0821e-01,\n",
              "         2.5907e-02,  1.3960e-01,  1.3254e-01, -1.6078e-01,  1.7432e-01,\n",
              "        -4.6780e-02,  1.5971e-01, -6.1331e-02, -1.4372e-01,  7.4853e-02,\n",
              "         1.1294e-01,  1.2167e-01, -1.1076e-01,  8.7318e-02, -5.8170e-02,\n",
              "        -2.3840e-02,  3.7629e-02,  1.1150e-01,  2.4353e-02,  1.3423e-01,\n",
              "        -7.6141e-02,  1.1195e-01,  6.8358e-02, -2.5095e-01, -7.2028e-03,\n",
              "         7.7966e-02, -6.9497e-02, -9.6484e-02,  1.2607e-01, -1.3162e-01,\n",
              "         8.7072e-02,  9.6802e-02, -1.0119e-01, -9.5707e-02, -2.5648e-01,\n",
              "        -4.5263e-02,  1.7528e-02, -1.5033e-02, -1.6359e-01,  2.2414e-02,\n",
              "         7.3366e-02, -8.9477e-03, -5.7556e-02, -1.0950e-02, -1.5172e-01,\n",
              "         5.0726e-02, -2.5911e-02, -3.4822e-02,  6.7676e-02, -2.8545e-02,\n",
              "         4.5194e-02, -2.5012e-02, -7.0625e-03,  1.6925e-01,  1.4922e-01,\n",
              "        -1.3122e-01, -6.6969e-02,  3.2825e-02,  2.4674e-02,  7.7112e-02,\n",
              "        -2.5684e-01,  6.4382e-02, -7.3266e-03,  7.4911e-02, -7.3007e-03,\n",
              "         2.4404e-03,  5.9835e-02, -7.4050e-02,  4.1262e-03, -1.0901e-01,\n",
              "        -2.1625e-01, -2.5622e-01, -1.8506e-01, -1.2901e-01, -4.6526e-02,\n",
              "         8.2895e-02,  2.2906e-03,  1.6445e-01, -2.7698e-02,  9.9889e-03,\n",
              "        -8.9750e-02,  2.4748e-02, -8.7560e-02,  1.0369e-02, -6.0028e-02,\n",
              "        -9.4751e-02,  1.1674e-01, -7.3175e-02, -1.2249e-01, -4.5053e-02,\n",
              "         1.2472e-01, -1.2611e-01, -1.4244e-01,  1.1656e-01,  1.5715e-01,\n",
              "        -3.2876e-02,  1.2129e-01,  7.6128e-02,  3.5388e-02,  4.7602e-02,\n",
              "         1.6380e-01, -7.2614e-02, -9.1284e-02,  4.7804e-02, -2.4025e-02,\n",
              "         1.4248e-01,  9.2824e-02, -1.0058e-01,  8.6550e-02,  1.6846e-01,\n",
              "        -2.2999e-01,  1.4498e-01, -7.1435e-02, -8.0483e-02, -1.1230e-01,\n",
              "        -1.0236e-01,  4.5862e-02,  7.6779e-02, -1.2166e-01,  1.3238e-02,\n",
              "        -1.9939e-02,  8.8260e-02,  1.2591e-01,  1.2431e-01, -1.1222e-01,\n",
              "         1.2275e-01, -2.0557e-01,  8.7008e-02, -1.2976e-01,  1.1021e-01,\n",
              "         1.2060e-01, -6.6300e-02,  1.0851e-02,  1.4633e-01,  2.0623e-02,\n",
              "        -1.5812e-01,  9.3091e-02,  6.4589e-02, -2.2974e-01, -1.2919e-01,\n",
              "         9.1557e-02,  1.0117e-01, -3.3901e-02,  3.3547e-02, -6.3514e-02,\n",
              "        -1.0747e-02, -1.1641e-01,  1.9327e-01, -2.5140e-02,  4.8835e-02,\n",
              "         1.2464e-01,  8.2228e-02,  1.8207e-01,  2.0343e-02,  3.5378e-02,\n",
              "        -5.0515e-03,  1.0781e-01,  9.0410e-02, -4.2833e-02, -1.2796e-01,\n",
              "        -2.2692e-03,  7.4566e-02, -4.9880e-03, -1.0896e-02, -2.9943e-02,\n",
              "        -6.7317e-02, -8.9812e-02,  1.7916e-02, -1.4892e-01, -6.6886e-03,\n",
              "        -6.8963e-02, -1.0803e-01,  6.7740e-02,  1.4278e-01,  6.4338e-02,\n",
              "         8.0533e-02, -1.6461e-01, -2.9097e-01,  8.7875e-02,  1.2188e-01,\n",
              "        -5.1809e-02, -1.1891e-01,  1.3667e-01,  1.9210e-01,  1.9959e-01,\n",
              "        -2.5164e-01,  1.6493e-01,  7.8079e-02,  1.4992e-01, -9.4752e-03,\n",
              "         1.4759e-02, -6.6029e-02,  4.9123e-02, -8.2437e-02, -6.6220e-03,\n",
              "        -5.1210e-02, -5.7382e-02, -1.2170e-01,  1.0297e-01, -3.1751e-02,\n",
              "        -2.4394e-02, -6.2359e-02,  6.1994e-02,  4.2702e-02, -7.7193e-02,\n",
              "        -2.9083e-02, -3.3909e-02, -5.9149e-03, -1.7683e-02,  5.8079e-02,\n",
              "        -5.8948e-02, -7.2073e-02,  1.4611e-02,  6.2695e-02,  4.1390e-02,\n",
              "        -1.6168e-02,  1.1246e-01,  5.0868e-02, -1.0886e-01,  9.3837e-02,\n",
              "        -1.9785e-01, -8.0000e-02,  1.7552e-01, -7.3100e-02,  9.5414e-02,\n",
              "         3.0550e-02, -3.2393e-02,  8.5009e-02,  1.7471e-01, -5.5705e-02,\n",
              "        -2.3312e-02,  1.0394e-02,  2.4470e-02, -1.1783e-01,  9.1722e-02,\n",
              "        -4.9348e-02, -1.8073e-01,  9.6826e-02, -5.6755e-01,  1.2614e-01,\n",
              "         2.9390e-03,  1.8769e-03, -1.1544e-01,  1.2663e-01, -5.8075e-02,\n",
              "         2.5107e-01,  1.1287e-01, -7.0834e-02, -1.6159e-02, -4.6640e-02,\n",
              "        -1.3588e-01, -9.3023e-02, -1.3563e-01, -2.1735e-02, -1.4699e-01,\n",
              "        -2.4624e-02,  1.0742e-01,  1.6097e-01,  4.5236e-02, -6.0530e-02,\n",
              "         2.5614e-02,  1.7842e-01, -1.5002e-01,  3.5617e-02, -1.0927e-02,\n",
              "         9.5441e-02, -5.1986e-02,  1.0888e-03, -5.7051e-02,  3.4985e-02,\n",
              "        -3.2108e-01, -1.7617e-01, -8.8610e-03,  1.1143e-02,  7.9310e-02,\n",
              "        -5.8020e-02, -2.9329e-02, -1.1323e-01, -1.2579e-01,  2.0275e-01,\n",
              "         3.5307e-04,  3.3124e-02,  6.7377e-02, -4.2155e-02, -1.1151e-01,\n",
              "        -8.8246e-02,  1.8438e-02, -7.8752e-02, -1.4929e-03, -1.0480e-01,\n",
              "         5.6258e-02, -8.0279e-02, -1.0454e-01, -1.4213e-01, -9.8723e-03,\n",
              "        -3.1789e-02,  1.3491e-02,  4.8246e-03, -1.2428e-01, -9.4912e-02,\n",
              "         7.5539e-02,  1.3727e-01, -2.4120e-03, -1.2110e-01,  6.5877e-02,\n",
              "         9.9069e-02, -1.6232e-03,  7.0322e-02, -1.3093e-01, -1.2612e-01,\n",
              "         2.8165e-02, -7.9214e-02,  1.3811e-01,  6.9900e-02,  3.6589e-02,\n",
              "         1.3032e-02,  8.1007e-02,  9.4133e-02,  3.9021e-02, -6.3269e-02,\n",
              "         6.4734e-02,  5.0746e-02, -6.4859e-02,  2.4542e-02,  7.7572e-02,\n",
              "        -8.3015e-02, -3.7940e-02, -1.0382e-01,  7.6982e-02,  1.0488e-01,\n",
              "         8.9766e-02, -2.2916e-02,  4.1690e-02,  1.6841e-01, -1.6387e-01,\n",
              "        -1.5209e-02,  8.3685e-02, -1.2062e-02, -1.0509e-02,  1.0691e+00,\n",
              "        -2.4897e-02, -1.4629e-01,  7.9504e-03, -1.5805e-02, -6.3116e-02,\n",
              "        -1.1533e-02, -4.7438e-02,  3.3857e-02, -1.0285e-01,  8.0286e-02,\n",
              "        -8.3017e-02,  3.6412e-01,  3.5055e-02,  1.1367e-01, -4.1308e-02,\n",
              "         3.7763e-02,  1.7893e-02, -1.1354e-02,  9.0176e-02,  1.8504e-01,\n",
              "         9.1006e-02, -2.4193e-02, -9.1733e-02,  6.1154e-02, -3.4899e-02,\n",
              "         3.9541e-02,  1.7300e-02,  7.5973e-02,  1.7125e-02,  1.8317e-01,\n",
              "         4.8119e-02, -5.6440e-02, -3.6205e-03,  1.7694e-02,  1.2270e-01,\n",
              "         9.0944e-02, -1.4055e-02,  6.9941e-02, -1.3236e-01, -9.3604e-03,\n",
              "         6.1123e-02,  1.8491e-01, -1.4185e-01,  4.5856e-02,  1.1677e-01,\n",
              "        -3.8666e-02, -1.3733e-01, -5.1623e-02, -1.1028e-01,  2.4513e-02,\n",
              "         4.1600e-02, -4.5819e-02, -1.2618e-01,  1.2042e-01, -1.5052e-01,\n",
              "        -1.3637e-01,  1.5062e-02,  4.5315e-02], grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(embedding_matrix) # Total tokens in the vocabulary pf GPT-2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bf8RIC3waSbp",
        "outputId": "f1fa6f45-dcde-40d3-e074-6115623f1e4a"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50257"
            ]
          },
          "metadata": {},
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"BPE BPE & WordPiece are tokenization methods used in GPT & BERT respectively.\"\n",
        "inputs = tokenizer(sentence, return_tensors='pt')\n",
        "input_ids = inputs['input_ids']\n",
        "input_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOghDWbuXocc",
        "outputId": "1b62cb4e-7fa5-4dcf-83a0-9b3b07c7192d"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[   33, 11401,   347, 11401,  1222,  9678,    47,  8535,   389, 11241,\n",
              "          1634,  5050,   973,   287,   402, 11571,  1222,   347, 17395,  8148,\n",
              "            13]])"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_sentence = tokenizer.decode([11571], skip_special_tokens=True)\n",
        "print(\"Decoded Sentence:\\n\", f'---{decoded_sentence}---')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86YVFTU5ZIN8",
        "outputId": "ed872281-bd57-48a6-8d6a-b64d5928a79a"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoded Sentence:\n",
            " ---PT---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Token embeddings\n",
        "token_embeddings = model.transformer.wte(input_ids)\n",
        "print(\"Token Embeddings:\\n\", token_embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jaKnwcwPX8ep",
        "outputId": "55b0932e-5c2c-4b8f-804f-ab564036bd95"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token Embeddings:\n",
            " tensor([[[ 0.1274, -0.1802,  0.1993,  ...,  0.0390, -0.1210, -0.0389],\n",
            "         [-0.0650, -0.0954,  0.1422,  ..., -0.0675, -0.1126,  0.0944],\n",
            "         [ 0.0282, -0.0954,  0.1376,  ...,  0.0116, -0.0064,  0.0582],\n",
            "         ...,\n",
            "         [ 0.0344,  0.0343,  0.0782,  ..., -0.2782, -0.1297,  0.1704],\n",
            "         [ 0.1531, -0.0402,  0.0887,  ...,  0.2619, -0.0958,  0.0981],\n",
            "         [ 0.0466, -0.0113,  0.0283,  ..., -0.0735,  0.0496,  0.0963]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(token_embeddings[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfF8qyj4a3Gt",
        "outputId": "8664521b-d861-4226-e98e-132686b15ee3"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_embeddings[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCz0dPUbay-q",
        "outputId": "a5494b60-7e73-4ea1-b727-b07e2964bfd2"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.1274, -0.1802,  0.1993,  ...,  0.0390, -0.1210, -0.0389],\n",
              "        [-0.0650, -0.0954,  0.1422,  ..., -0.0675, -0.1126,  0.0944],\n",
              "        [ 0.0282, -0.0954,  0.1376,  ...,  0.0116, -0.0064,  0.0582],\n",
              "        ...,\n",
              "        [ 0.0344,  0.0343,  0.0782,  ..., -0.2782, -0.1297,  0.1704],\n",
              "        [ 0.1531, -0.0402,  0.0887,  ...,  0.2619, -0.0958,  0.0981],\n",
              "        [ 0.0466, -0.0113,  0.0283,  ..., -0.0735,  0.0496,  0.0963]],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "def decode_embedding_to_token(embedding, model, tokenizer):\n",
        "    embedding_matrix = model.transformer.wte.weight\n",
        "    similarities = F.cosine_similarity(embedding, embedding_matrix, dim=-1)\n",
        "    token_id = torch.argmax(similarities).item()\n",
        "    token = tokenizer.decode([token_id])\n",
        "    return token, token_id"
      ],
      "metadata": {
        "id": "nccu42vEbOBW"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding = token_embeddings[0][1][:]\n",
        "token, token_id = decode_embedding_to_token(embedding, model, tokenizer)\n",
        "\n",
        "print(f\"Decoded Token: {token}\")\n",
        "print(f\"Token ID: {token_id}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LPc5Eaf1bBMc",
        "outputId": "f3071960-f8bd-4cb8-fdcf-35d8ca1768ad"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoded Token: PE\n",
            "Token ID: 11401\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Positional encodings\n",
        "position_ids = torch.arange(0, input_ids.size(-1), dtype=torch.long).unsqueeze(0)\n",
        "position_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egLtj4nYYDVU",
        "outputId": "6f43d7a6-7211-49b3-aa13-b247392a3f90"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
              "         18, 19, 20]])"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "positional_encodings = model.transformer.wpe(position_ids)\n",
        "print(\"Positional Encodings:\\n\", positional_encodings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I7Fa4nW9t6su",
        "outputId": "4426cfff-a139-4865-b7f6-0da0cc7d40d9"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positional Encodings:\n",
            " tensor([[[-1.8821e-02, -1.9742e-01,  4.0267e-03,  ..., -4.3044e-02,\n",
            "           2.8267e-02,  5.4490e-02],\n",
            "         [ 2.3959e-02, -5.3792e-02, -9.4879e-02,  ...,  3.4170e-02,\n",
            "           1.0172e-02, -1.5573e-04],\n",
            "         [ 4.2161e-03, -8.4764e-02,  5.4515e-02,  ...,  1.9745e-02,\n",
            "           1.9325e-02, -2.1424e-02],\n",
            "         ...,\n",
            "         [ 7.4972e-04,  2.8626e-02,  7.5494e-02,  ..., -3.7352e-03,\n",
            "          -2.5456e-03, -2.7157e-03],\n",
            "         [-6.7148e-03,  3.1997e-02,  8.2699e-02,  ..., -4.1213e-03,\n",
            "          -4.8707e-03, -1.1040e-03],\n",
            "         [-2.3811e-04,  2.6748e-02,  6.9952e-02,  ..., -1.5740e-04,\n",
            "          -3.6072e-03, -1.2081e-03]]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(positional_encodings[0][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2Fkbz1xcUki",
        "outputId": "d89101b0-11e4-420e-8da7-c7307f3076c1"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "768"
            ]
          },
          "metadata": {},
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_positional_embedding(positional_embedding, model):\n",
        "    positional_embedding_matrix = model.transformer.wpe.weight\n",
        "    similarities = F.cosine_similarity(positional_embedding, positional_embedding_matrix, dim=-1)\n",
        "    position = torch.argmax(similarities).item()\n",
        "    return position"
      ],
      "metadata": {
        "id": "Aif0QTlRcn77"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positional_embedding = positional_encodings[0][0][:]\n",
        "position = decode_positional_embedding(positional_embedding, model)\n",
        "\n",
        "print(f\"Decoded Position: {position}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_KhwGIZb_gx",
        "outputId": "940a99f3-6d54-417f-cd56-031005ce0996"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoded Position: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def get_positional_encoding(seq_len, d_model):\n",
        "    positional_encoding = np.zeros((seq_len, d_model))\n",
        "    for pos in range(seq_len):\n",
        "        for i in range(0, d_model, 2):\n",
        "            positional_encoding[pos, i] = np.sin(pos / (10000 ** ((2 * i) / d_model)))\n",
        "            if i + 1 < d_model:\n",
        "                positional_encoding[pos, i + 1] = np.cos(pos / (10000 ** ((2 * i) / d_model)))\n",
        "\n",
        "    return torch.tensor(positional_encoding, dtype=torch.float32)\n",
        "\n",
        "seq_len = 21\n",
        "d_model = 768\n",
        "positional_encoding = get_positional_encoding(seq_len, d_model)\n",
        "print(positional_encoding)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ky4oHLhgq0xK",
        "outputId": "3040e62f-732d-46b2-e30f-ac5aeaec5fb1"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  ...,  1.0000e+00,\n",
            "          0.0000e+00,  1.0000e+00],\n",
            "        [ 8.4147e-01,  5.4030e-01,  8.1525e-01,  ...,  1.0000e+00,\n",
            "          1.0491e-08,  1.0000e+00],\n",
            "        [ 9.0930e-01, -4.1615e-01,  9.4424e-01,  ...,  1.0000e+00,\n",
            "          2.0983e-08,  1.0000e+00],\n",
            "        ...,\n",
            "        [-7.5099e-01,  6.6032e-01, -9.9259e-01,  ...,  1.0000e+00,\n",
            "          1.8885e-07,  1.0000e+00],\n",
            "        [ 1.4988e-01,  9.8870e-01, -6.7390e-01,  ...,  1.0000e+00,\n",
            "          1.9934e-07,  1.0000e+00],\n",
            "        [ 9.1295e-01,  4.0808e-01,  2.1206e-01,  ...,  1.0000e+00,\n",
            "          2.0983e-07,  1.0000e+00]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine token embeddings and positional encodings\n",
        "combined_embeddings = token_embeddings + positional_encodings\n",
        "print(\"Combined Embeddings:\\n\", combined_embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7PyjpxtYEfs",
        "outputId": "1ceffd14-03f0-40d3-ce0e-3e7c009efdc1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined Embeddings:\n",
            " tensor([[[-0.0874, -0.2177,  0.0685,  ...,  0.0208,  0.0183,  0.0576],\n",
            "         [ 0.0156, -0.0840,  0.1020,  ...,  0.0561, -0.0880, -0.0073],\n",
            "         [-0.0475,  0.0343,  0.1346,  ...,  0.1119,  0.3051,  0.1437],\n",
            "         ...,\n",
            "         [ 0.1248,  0.0443,  0.2869,  ...,  0.0733, -0.0299, -0.0757],\n",
            "         [ 0.0932, -0.0789,  0.1457,  ..., -0.1400,  0.1534,  0.0305],\n",
            "         [ 0.0516, -0.0092,  0.1461,  ..., -0.0738,  0.0537,  0.0908]]],\n",
            "       grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7R_YmBG7Ac38"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}